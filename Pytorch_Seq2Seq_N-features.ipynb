{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### library setttings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# others\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "from copy import deepcopy # Add Deepcopy for args\n",
    "import pickle \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (15, 9)\n",
    "\n",
    "\n",
    "# read file\n",
    "file_path = './data/merged_data.h5'\n",
    "data = pd.read_hdf(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>time</th>\n",
       "      <th>coin_index</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>quote_av</th>\n",
       "      <th>trades</th>\n",
       "      <th>tb_base_av</th>\n",
       "      <th>tb_quote_av</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.983614</td>\n",
       "      <td>0.983614</td>\n",
       "      <td>0.983128</td>\n",
       "      <td>0.983246</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>10.650987</td>\n",
       "      <td>0.009855</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>6.771755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.983245</td>\n",
       "      <td>0.983612</td>\n",
       "      <td>0.982453</td>\n",
       "      <td>0.982693</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>11.375689</td>\n",
       "      <td>0.016137</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>5.565188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.982694</td>\n",
       "      <td>0.983612</td>\n",
       "      <td>0.982403</td>\n",
       "      <td>0.983002</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>12.301942</td>\n",
       "      <td>0.014166</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>7.225459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.983009</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.983009</td>\n",
       "      <td>0.984486</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>20.134695</td>\n",
       "      <td>0.021557</td>\n",
       "      <td>0.001171</td>\n",
       "      <td>9.353000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.984233</td>\n",
       "      <td>0.984606</td>\n",
       "      <td>0.983612</td>\n",
       "      <td>0.984164</td>\n",
       "      <td>0.002818</td>\n",
       "      <td>22.515448</td>\n",
       "      <td>0.021434</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>14.372534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id  time  coin_index      open      high       low     close  \\\n",
       "0          0     0           9  0.983614  0.983614  0.983128  0.983246   \n",
       "1          0     1           9  0.983245  0.983612  0.982453  0.982693   \n",
       "2          0     2           9  0.982694  0.983612  0.982403  0.983002   \n",
       "3          0     3           9  0.983009  0.984848  0.983009  0.984486   \n",
       "4          0     4           9  0.984233  0.984606  0.983612  0.984164   \n",
       "\n",
       "     volume   quote_av    trades  tb_base_av  tb_quote_av  \n",
       "0  0.001334  10.650987  0.009855    0.000848     6.771755  \n",
       "1  0.001425  11.375689  0.016137    0.000697     5.565188  \n",
       "2  0.001542  12.301942  0.014166    0.000905     7.225459  \n",
       "3  0.002520  20.134695  0.021557    0.001171     9.353000  \n",
       "4  0.002818  22.515448  0.021434    0.001799    14.372534  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 정규화 및 스플릿 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datascaling_and_spliting(dataframe):\n",
    "    '''  scaling version2 : 거래량 데이터외의 데이터도 넣을 경우 '''\n",
    "    \n",
    "    n = data.sample_id.nunique()\n",
    "\n",
    "    # train, val, test = 7 : 2  : 1\n",
    "    train_df = data[data.sample_id < int(n*0.7)]\n",
    "    val_df =data[(data.sample_id >= int(n*0.7)) & (data.sample_id < int(n*0.9))]\n",
    "    test_df = data[data.sample_id >= int(n*0.9)]\n",
    "    \n",
    "    # create scaler and dafaframes\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # scaler fitting\n",
    "    scaler = scaler.fit(train_df.iloc[:, -5:])\n",
    "    \n",
    "    # transforms each dataframes\n",
    "    train_df.iloc[:, -5:] = scaler.transform(train_df.iloc[:, -5:])\n",
    "    val_df.iloc[:, -5:] = scaler.transform(val_df.iloc[:, -5:])\n",
    "    test_df.iloc[:, -5:] = scaler.transform(test_df.iloc[:, -5:])\n",
    "\n",
    "    return train_df, val_df, test_df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\venv\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Anaconda3\\envs\\venv\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Anaconda3\\envs\\venv\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n"
     ]
    }
   ],
   "source": [
    "scaled_train_df, scaled_val_df, scaled_test_df, scaler = datascaling_and_spliting(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>time</th>\n",
       "      <th>coin_index</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>quote_av</th>\n",
       "      <th>trades</th>\n",
       "      <th>tb_base_av</th>\n",
       "      <th>tb_quote_av</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1630500</th>\n",
       "      <td>1087</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1.010196</td>\n",
       "      <td>1.011977</td>\n",
       "      <td>1.008776</td>\n",
       "      <td>1.009570</td>\n",
       "      <td>0.019519</td>\n",
       "      <td>0.023766</td>\n",
       "      <td>0.040007</td>\n",
       "      <td>0.011853</td>\n",
       "      <td>0.012426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630501</th>\n",
       "      <td>1087</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1.009476</td>\n",
       "      <td>1.010790</td>\n",
       "      <td>1.007363</td>\n",
       "      <td>1.009166</td>\n",
       "      <td>0.015788</td>\n",
       "      <td>0.019197</td>\n",
       "      <td>0.045144</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.012481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630502</th>\n",
       "      <td>1087</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1.009166</td>\n",
       "      <td>1.009839</td>\n",
       "      <td>1.007852</td>\n",
       "      <td>1.009602</td>\n",
       "      <td>0.015113</td>\n",
       "      <td>0.018377</td>\n",
       "      <td>0.041034</td>\n",
       "      <td>0.013681</td>\n",
       "      <td>0.014323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630503</th>\n",
       "      <td>1087</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1.009602</td>\n",
       "      <td>1.009879</td>\n",
       "      <td>1.007869</td>\n",
       "      <td>1.009652</td>\n",
       "      <td>0.021918</td>\n",
       "      <td>0.026666</td>\n",
       "      <td>0.039429</td>\n",
       "      <td>0.023879</td>\n",
       "      <td>0.025008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630504</th>\n",
       "      <td>1087</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1.009614</td>\n",
       "      <td>1.011977</td>\n",
       "      <td>1.009363</td>\n",
       "      <td>1.011740</td>\n",
       "      <td>0.013460</td>\n",
       "      <td>0.016386</td>\n",
       "      <td>0.035897</td>\n",
       "      <td>0.014611</td>\n",
       "      <td>0.015312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sample_id  time  coin_index      open      high       low     close  \\\n",
       "1630500       1087     0           9  1.010196  1.011977  1.008776  1.009570   \n",
       "1630501       1087     1           9  1.009476  1.010790  1.007363  1.009166   \n",
       "1630502       1087     2           9  1.009166  1.009839  1.007852  1.009602   \n",
       "1630503       1087     3           9  1.009602  1.009879  1.007869  1.009652   \n",
       "1630504       1087     4           9  1.009614  1.011977  1.009363  1.011740   \n",
       "\n",
       "           volume  quote_av    trades  tb_base_av  tb_quote_av  \n",
       "1630500  0.019519  0.023766  0.040007    0.011853     0.012426  \n",
       "1630501  0.015788  0.019197  0.045144    0.011924     0.012481  \n",
       "1630502  0.015113  0.018377  0.041034    0.013681     0.014323  \n",
       "1630503  0.021918  0.026666  0.039429    0.023879     0.025008  \n",
       "1630504  0.013460  0.016386  0.035897    0.014611     0.015312  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규화 확인\n",
    "scaled_test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 generator 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerater(Dataset):\n",
    "    ''' 설명 생략 '''\n",
    "    \n",
    "    def __init__(self, dataframe, x_frames, y_frames, status_idx = None):\n",
    "\n",
    "        self.grouped_dataframe = dataframe.groupby('sample_id')\n",
    "        self.sample_length = dataframe.sample_id.nunique()\n",
    "        self.x_frames = x_frames\n",
    "        self.y_frames = y_frames\n",
    "        \n",
    "        if status_idx == 'train':\n",
    "            self.status_idx = 0\n",
    "            print(self.status_idx)\n",
    "            \n",
    "        elif status_idx == 'validation':\n",
    "            self.status_idx = 845\n",
    "            print(self.status_idx)\n",
    "            \n",
    "        else:\n",
    "            self.status_idx = 1087\n",
    "            print(self.status_idx)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.sample_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        idx += self.status_idx\n",
    "        \n",
    "        one_sample_df = self.grouped_dataframe.get_group(idx)\n",
    "        \n",
    "        one_sample_df = one_sample_df[-(self.x_frames + self.y_frames):]\n",
    "        one_sample_arr = one_sample_df.drop(['sample_id','time', 'coin_index'], axis = 1).values\n",
    "\n",
    "        # get values\n",
    "        X = one_sample_arr[:self.x_frames, :]\n",
    "        y = one_sample_arr[self.x_frames:, :]\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hpyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n",
      "0\n",
      "845\n",
      "1087\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ====== initialization\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device is\",args.device)\n",
    "\n",
    "seed = 777\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# ====== Model Capacity options ===== #\n",
    "args.input_dim = 9\n",
    "args.hidden_dim = 50\n",
    "args.output_dim = 1\n",
    "args.num_layers = 3\n",
    "args.batch_size = 24\n",
    "args.dropout = 0.2\n",
    "args.target_len = 120\n",
    "args.use_bn = True\n",
    "\n",
    "# ====== Dataset Generating options ====== #\n",
    "args.x_frames = 255\n",
    "args.y_frames = 120\n",
    "\n",
    "# ====== Model training options ===== #\n",
    "args.num_epoch = 30\n",
    "args.learning_rate = 0.001\n",
    "args.L2_rate = 0.0001\n",
    "args.training_prediction = 'mixed_teacher_forcing'\n",
    "args.teacher_forcing_ratio = 0.6\n",
    "\n",
    "\n",
    "# dataset partitioning\n",
    "trainset = DatasetGenerater(scaled_train_df, x_frames = args.x_frames, y_frames = args.y_frames, status_idx = 'train')\n",
    "valset = DatasetGenerater(scaled_val_df, x_frames = args.x_frames, y_frames = args.y_frames, status_idx = 'validation')\n",
    "testset = DatasetGenerater(scaled_test_df, x_frames = args.x_frames, y_frames = args.y_frames, status_idx = 'test')\n",
    "partition = {'train': trainset, 'val':valset, 'test':testset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>time</th>\n",
       "      <th>coin_index</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>quote_av</th>\n",
       "      <th>trades</th>\n",
       "      <th>tb_base_av</th>\n",
       "      <th>tb_quote_av</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1378</th>\n",
       "      <td>0</td>\n",
       "      <td>1378</td>\n",
       "      <td>9</td>\n",
       "      <td>0.999629</td>\n",
       "      <td>1.000116</td>\n",
       "      <td>0.999143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010375</td>\n",
       "      <td>0.012053</td>\n",
       "      <td>0.011322</td>\n",
       "      <td>0.010341</td>\n",
       "      <td>0.010341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379</th>\n",
       "      <td>0</td>\n",
       "      <td>1379</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000123</td>\n",
       "      <td>0.999388</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.003489</td>\n",
       "      <td>0.007992</td>\n",
       "      <td>0.003344</td>\n",
       "      <td>0.003345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>0</td>\n",
       "      <td>1380</td>\n",
       "      <td>9</td>\n",
       "      <td>0.999389</td>\n",
       "      <td>0.999630</td>\n",
       "      <td>0.999261</td>\n",
       "      <td>0.999385</td>\n",
       "      <td>0.002810</td>\n",
       "      <td>0.003264</td>\n",
       "      <td>0.006793</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.002224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381</th>\n",
       "      <td>0</td>\n",
       "      <td>1381</td>\n",
       "      <td>9</td>\n",
       "      <td>0.999593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999265</td>\n",
       "      <td>0.999266</td>\n",
       "      <td>0.009934</td>\n",
       "      <td>0.011539</td>\n",
       "      <td>0.011122</td>\n",
       "      <td>0.011090</td>\n",
       "      <td>0.011088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1382</th>\n",
       "      <td>0</td>\n",
       "      <td>1382</td>\n",
       "      <td>9</td>\n",
       "      <td>0.999266</td>\n",
       "      <td>0.999618</td>\n",
       "      <td>0.999262</td>\n",
       "      <td>0.999322</td>\n",
       "      <td>0.009332</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>0.008991</td>\n",
       "      <td>0.010919</td>\n",
       "      <td>0.010914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267495</th>\n",
       "      <td>844</td>\n",
       "      <td>1495</td>\n",
       "      <td>9</td>\n",
       "      <td>1.002049</td>\n",
       "      <td>1.003136</td>\n",
       "      <td>1.001533</td>\n",
       "      <td>1.002579</td>\n",
       "      <td>0.006706</td>\n",
       "      <td>0.022019</td>\n",
       "      <td>0.037540</td>\n",
       "      <td>0.004785</td>\n",
       "      <td>0.013523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267496</th>\n",
       "      <td>844</td>\n",
       "      <td>1496</td>\n",
       "      <td>9</td>\n",
       "      <td>1.002579</td>\n",
       "      <td>1.003686</td>\n",
       "      <td>1.001733</td>\n",
       "      <td>1.003645</td>\n",
       "      <td>0.004476</td>\n",
       "      <td>0.014699</td>\n",
       "      <td>0.019656</td>\n",
       "      <td>0.004404</td>\n",
       "      <td>0.012451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267497</th>\n",
       "      <td>844</td>\n",
       "      <td>1497</td>\n",
       "      <td>9</td>\n",
       "      <td>1.003636</td>\n",
       "      <td>1.003919</td>\n",
       "      <td>1.001591</td>\n",
       "      <td>1.001806</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>0.011099</td>\n",
       "      <td>0.021097</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>0.007354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267498</th>\n",
       "      <td>844</td>\n",
       "      <td>1498</td>\n",
       "      <td>9</td>\n",
       "      <td>1.001806</td>\n",
       "      <td>1.001881</td>\n",
       "      <td>0.998480</td>\n",
       "      <td>0.999429</td>\n",
       "      <td>0.014149</td>\n",
       "      <td>0.046351</td>\n",
       "      <td>0.053038</td>\n",
       "      <td>0.007034</td>\n",
       "      <td>0.019832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267499</th>\n",
       "      <td>844</td>\n",
       "      <td>1499</td>\n",
       "      <td>9</td>\n",
       "      <td>0.999429</td>\n",
       "      <td>1.001260</td>\n",
       "      <td>0.998026</td>\n",
       "      <td>1.000732</td>\n",
       "      <td>0.012252</td>\n",
       "      <td>0.040124</td>\n",
       "      <td>0.047415</td>\n",
       "      <td>0.011320</td>\n",
       "      <td>0.031911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1266122 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sample_id  time  coin_index      open      high       low     close  \\\n",
       "1378             0  1378           9  0.999629  1.000116  0.999143  1.000000   \n",
       "1379             0  1379           9  1.000000  1.000123  0.999388  0.999998   \n",
       "1380             0  1380           9  0.999389  0.999630  0.999261  0.999385   \n",
       "1381             0  1381           9  0.999593  1.000000  0.999265  0.999266   \n",
       "1382             0  1382           9  0.999266  0.999618  0.999262  0.999322   \n",
       "...            ...   ...         ...       ...       ...       ...       ...   \n",
       "1267495        844  1495           9  1.002049  1.003136  1.001533  1.002579   \n",
       "1267496        844  1496           9  1.002579  1.003686  1.001733  1.003645   \n",
       "1267497        844  1497           9  1.003636  1.003919  1.001591  1.001806   \n",
       "1267498        844  1498           9  1.001806  1.001881  0.998480  0.999429   \n",
       "1267499        844  1499           9  0.999429  1.001260  0.998026  1.000732   \n",
       "\n",
       "           volume  quote_av    trades  tb_base_av  tb_quote_av  \n",
       "1378     0.010375  0.012053  0.011322    0.010341     0.010341  \n",
       "1379     0.003003  0.003489  0.007992    0.003344     0.003345  \n",
       "1380     0.002810  0.003264  0.006793    0.002225     0.002224  \n",
       "1381     0.009934  0.011539  0.011122    0.011090     0.011088  \n",
       "1382     0.009332  0.010837  0.008991    0.010919     0.010914  \n",
       "...           ...       ...       ...         ...          ...  \n",
       "1267495  0.006706  0.022019  0.037540    0.004785     0.013523  \n",
       "1267496  0.004476  0.014699  0.019656    0.004404     0.012451  \n",
       "1267497  0.003378  0.011099  0.021097    0.002601     0.007354  \n",
       "1267498  0.014149  0.046351  0.053038    0.007034     0.019832  \n",
       "1267499  0.012252  0.040124  0.047415    0.011320     0.031911  \n",
       "\n",
       "[1266122 rows x 12 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_train_df[1378:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 255, 9]) torch.Size([24, 120, 9])\n"
     ]
    }
   ],
   "source": [
    "trainloader = DataLoader(partition['train'], batch_size = args.batch_size, shuffle = True, drop_last = True)\n",
    "\n",
    "for i, (X,y) in enumerate(trainloader):\n",
    "    print(X.shape, y.shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    ''' Encodes time-serise sequence '''\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        \n",
    "        '''\n",
    "        : param input_size:     the number of features in the input X\n",
    "        : param hidden_size:    the number of features in the hidden state h\n",
    "        : param num_layers:     number of recurrent layers (i.e., 2 means there are 2 stacked LSTMs)\n",
    "        '''\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        \n",
    "        # define LSTM layer\n",
    "        self.rnn = nn.GRU(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \n",
    "        '''\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence;\n",
    "        :                              hidden gives the hidden state and cell state for the last\n",
    "        :                              element in the sequence \n",
    "        '''\n",
    "        \n",
    "        rnn_out, self.hidden = self.rnn(X)\n",
    "        return rnn_out, self.hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        '''\n",
    "        initialize hidden state\n",
    "        : param batch_size:    x_input.shape[1]\n",
    "        : return:              zeroed hidden state and cell state \n",
    "        '''\n",
    "        \n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_dim))\n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    ''' Decodes hidden state output by encoder '''\n",
    "    \n",
    "    def __init__(self, output_dim, hidden_dim, num_layers, dropout, use_bn):\n",
    "        \n",
    "        '''\n",
    "        : param output_dim:     the number of features in the input X\n",
    "        : param hidden_dim:    the number of features in the hidden state h\n",
    "        : param num_layers:     number of recurrent layers (i.e., 2 means there are\n",
    "        :                       2 stacked LSTMs)\n",
    "        '''\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim = output_dim \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.use_bn = use_bn\n",
    "        \n",
    "        self.rnn = nn.GRU(self.output_dim, self.hidden_dim, self.num_layers)\n",
    "        self.fc_out = self.regressor()\n",
    "        \n",
    "    def forward(self, x_input, encoder_hidden_states):\n",
    "               \n",
    "        '''        \n",
    "        : param x_input:                    should be 2D (batch_size, input_size)\n",
    "        : param encoder_hidden_states:      hidden states\n",
    "        : return output, hidden:            output gives all the hidden states in the sequence;\n",
    "        :                                   hidden gives the hidden state and cell state for the last\n",
    "        :                                   element in the sequence \n",
    " \n",
    "        '''\n",
    "        \n",
    "        # print(f'decoder input size {x_input.shape} to {x_input.reshape(1, -1, self.output_dim).shape}')\n",
    "        rnn_out, self.hidden = self.rnn(x_input.reshape(1, -1, self.output_dim), encoder_hidden_states)\n",
    "        output = self.fc_out(rnn_out.squeeze(0)) \n",
    "        \n",
    "        return output, self.hidden\n",
    "    \n",
    "    def regressor(self):\n",
    "\n",
    "        layers = []\n",
    "        if self.use_bn:\n",
    "            layers.append(nn.BatchNorm1d(self.hidden_dim))\n",
    "        layers.append(nn.Dropout(self.dropout))\n",
    "        \n",
    "        layers.append(nn.Linear(self.hidden_dim, self.hidden_dim // 2))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(self.hidden_dim // 2, self.output_dim))\n",
    "        regressor = nn.Sequential(*layers)\n",
    "\n",
    "        return regressor\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device, target_len, training_prediction, teacher_forcing_ratio):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.target_len = target_len\n",
    "        self.training_prediction = training_prediction\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        \n",
    "        assert encoder.hidden_dim == decoder.hidden_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.num_layers == decoder.num_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    \n",
    "    def forward(self, X, y):\n",
    "            \n",
    "            if y is None:\n",
    "                self.training_prediction = 'recursive'\n",
    "                \n",
    "                \n",
    "            # initialize\n",
    "            outputs = torch.zeros(self.target_len, X.shape[1], self.decoder.output_dim).to(self.device)\n",
    "            \n",
    "            encoder_hidden = self.encoder.init_hidden(X.shape[1])\n",
    "        \n",
    "            # encoder process\n",
    "            encoder_output, encoder_hidden = self.encoder(X)  \n",
    "            \n",
    "            #decoder process\n",
    "            ''' last X sequence, shape = [batch_size, open_index = 0] '''    \n",
    "            decoder_input = X[-1, :, 0]  \n",
    "            decoder_hidden = encoder_hidden  \n",
    "            \n",
    "\n",
    "            # ======================================================================================#\n",
    "            if self.training_prediction == 'recursive':\n",
    "                # predict recursively\n",
    "                for t in range(self.target_len): \n",
    "                    decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                    \n",
    "                    outputs[t] = decoder_output \n",
    "                    decoder_input = decoder_output\n",
    "                    \n",
    "           # ======================================================================================#\n",
    "            if self.training_prediction == 'teacher_forcing':\n",
    "                # use teacher forcing\n",
    "                if random.random() < self.teacher_forcing_ratio:\n",
    "                    for t in range(self.target_len): \n",
    "                        decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                        outputs[t] = decoder_output\n",
    "                        decoder_input = y[t, :].unsqueeze(1)\n",
    "                        \n",
    "                # predict recursively \n",
    "                else:\n",
    "                    for t in range(self.target_len): \n",
    "                        decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                        outputs[t] = decoder_output\n",
    "                        decoder_input = decoder_output\n",
    "                        \n",
    "            # ======================================================================================#\n",
    "            if self.training_prediction == 'mixed_teacher_forcing':\n",
    "                # predict using mixed teacher forcing\n",
    "                for t in range(self.target_len):\n",
    "\n",
    "                    decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                    outputs[t] = decoder_output \n",
    "                                    \n",
    "                    # predict with teacher forcing\n",
    "                    if random.random() < self.teacher_forcing_ratio:\n",
    "                        decoder_input = y[t, :].unsqueeze(1)\n",
    "\n",
    "                    # predict recursively \n",
    "                    else:\n",
    "                        decoder_input = decoder_output\n",
    "\n",
    "            # ======================================================================================#      \n",
    "            return outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120, 24]) torch.Size([120, 24, 1])\n",
      "torch.Size([120, 24]) torch.Size([120, 24, 1])\n",
      "torch.Size([120, 24]) torch.Size([120, 24, 1])\n",
      "torch.Size([120, 24]) torch.Size([120, 24, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-fc66e8b5f6e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m#backpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\venv\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "trainloader = DataLoader(partition['train'], batch_size = args.batch_size, shuffle = False, drop_last = True)\n",
    "\n",
    "# Encoder\n",
    "enc = Encoder(args.input_dim, args.hidden_dim, args.num_layers)\n",
    "\n",
    "# Decoder\n",
    "dec = Decoder(args.output_dim, args.hidden_dim, args.num_layers, args.dropout, args.use_bn)\n",
    "\n",
    "# Seq2Seq model\n",
    "model = Seq2Seq(enc, dec, args.device, args.target_len, args.training_prediction, args.teacher_forcing_ratio)\n",
    "model.to(args.device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.L2_rate)\n",
    "\n",
    "# model's mode setting\n",
    "model.train()\n",
    "\n",
    "# model initialization\n",
    "model.zero_grad()\n",
    "train_loss = 0.0\n",
    "\n",
    "# train\n",
    "for i, (X, y) in enumerate(trainloader):\n",
    "\n",
    "    X = X.transpose(0, 1).float().to(args.device)\n",
    "\n",
    "    # y = y[:,:, 0].transpose(0, 1).float().to(args.device)\n",
    "    y_true = y[:,:, 0].transpose(0, 1).float().to(args.device)\n",
    "    \n",
    "    # zero the gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # en-decoder outputs tensor \n",
    "    y_pred = model(X, y_true)\n",
    "    \n",
    "    \n",
    "    # compute the loss \n",
    "    loss = loss_fn(y_true.unsqueeze(2), y_pred)\n",
    "\n",
    "    #backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # get the batch loss\n",
    "    train_loss += loss.item()\n",
    "\n",
    "train_loss = train_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, partition, optimizer, loss_fn, args):\n",
    "    ''' model training '''\n",
    "    \n",
    "   \n",
    "    # data load\n",
    "    trainloader = DataLoader(partition['train'], batch_size = args.batch_size, shuffle = True, drop_last = True)\n",
    "    \n",
    "    # model's mode setting\n",
    "    model.train()\n",
    "\n",
    "    # model initialization\n",
    "    model.zero_grad()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # train\n",
    "    for i, (X, y) in enumerate(trainloader):\n",
    "        \n",
    "        X = X.transpose(0, 1).float().to(args.device)\n",
    "        y_true = y[:,:, 0].transpose(0, 1).float().to(args.device)\n",
    "\n",
    "        # zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # en-decoder outputs tensor \n",
    "        y_pred = model(X, y_true)\n",
    "\n",
    "        # compute the loss \n",
    "        loss = loss_fn(y_true, y_pred.squeeze(2))\n",
    "        \n",
    "        #backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # get the batch loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    return model, train_loss\n",
    "\n",
    "\n",
    "def validate(model, partition, loss_fn, args):\n",
    "    ''' model validate '''\n",
    "    \n",
    "    # data load\n",
    "    valloader = DataLoader(partition['val'], batch_size = args.batch_size, shuffle = False, drop_last = True)\n",
    "    \n",
    "    # model's mode setting\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    # evaluate\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(valloader):\n",
    "            \n",
    "            X = X.transpose(0, 1).float().to(args.device)\n",
    "            y_true = y[:, :, 0].transpose(0, 1).float().to(args.device)\n",
    "\n",
    "            # en-decoder outputs tensor \n",
    "            y_pred = model(X, None)\n",
    "            \n",
    "            # compute the loss \n",
    "            loss = loss_fn(y_true, y_pred.squeeze(2))\n",
    "\n",
    "            # get the batch loss\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss = val_loss / len(valloader)\n",
    " \n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def test(model, partition, scaler, args):\n",
    "    ''' model test '''\n",
    "    \n",
    "    # data load\n",
    "    testloader = DataLoader(partition['test'], batch_size = 1, shuffle = False, drop_last = False)\n",
    "    \n",
    "    # model's mode setting\n",
    "    model.eval()\n",
    "    \n",
    "    test_mae = 0.0\n",
    "    score_list = list()\n",
    "    item_loss_list = list()\n",
    "    \n",
    "    # evaluate\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(testloader):\n",
    "            \n",
    "            X = X.transpose(0, 1).float().to(args.device)            \n",
    "            y_true = y[:, :, 0].transpose(0, 1).float().to(args.device)\n",
    "            \n",
    "            # en-decoder outputs tensor \n",
    "            y_pred = model(X, None)\n",
    "            y_pred = y_pred.squeeze(2)\n",
    "            \n",
    "            # y values to cpu\n",
    "            y_true = y_true.cpu().detach().numpy()\n",
    "            y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "            # get the batch loss\n",
    "            test_mae += mean_absolute_error(y_true, y_pred)\n",
    "#             score = r2_score(y_pred = y_pred.transpose(), y_true = y_true.transpose(), multioutput = 'uniform_average')\n",
    "#             score_list.append(score)\n",
    "            item_loss_list.append(abs(y_true - y_pred))\n",
    "            \n",
    "            \n",
    "    test_mae /= len(testloader)\n",
    "#     score /= len(testloader)\n",
    "    \n",
    "    return test_mae, item_loss_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(partition, scaler, args):\n",
    "\n",
    "    # Encoder\n",
    "    enc = Encoder(args.input_dim, args.hidden_dim, args.num_layers)\n",
    "    \n",
    "    # Decoder\n",
    "    dec = Decoder(args.output_dim, args.hidden_dim, args.num_layers, args.dropout, args.use_bn)\n",
    "    \n",
    "    # Seq2Seq model\n",
    "    model = Seq2Seq(enc, dec, args.device, args.target_len, args.training_prediction, args.teacher_forcing_ratio)\n",
    "    model.to(args.device)\n",
    "    \n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.L2_rate)\n",
    "    \n",
    "    # epoch-wise loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(args.num_epoch):\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        model, train_loss = train(model, partition, optimizer, loss_fn, args)\n",
    "        val_loss = validate(model, partition, loss_fn, args)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # add epoch loss\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print('Epoch {},Loss(train/val) {:.3f}/{:.3f}. Took {:.2f} sec'.format(epoch+1, train_loss, val_loss, end_time-start_time))\n",
    "    \n",
    "    # test part\n",
    "    # test_mae, item_loss_list = test(model, partition, scaler, args)\n",
    "    \n",
    "    # ======= Add Result to Dictionary ======= #\n",
    "    result = {}\n",
    "    \n",
    "    result['train_losses'] = train_losses #epoch 수에 의존\n",
    "    result['val_losses'] = val_losses \n",
    "    \n",
    "    #result['test_mae'] = test_mae.round(3).item()\n",
    "    \n",
    "    # result['r2'] = np.array(score_list).mean().round(3)\n",
    "    # item_loss = np.array(item_loss_list).mean(axis=0).mean(axis=0).astype(int)\n",
    "    # item_loss = list([int(x) for x in item_loss])\n",
    "    # result['item_loss'] = item_loss\n",
    "     \n",
    "    return vars(args), result, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,Loss(train/val) 0.692/71.133. Took 36.70 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-98a8e9e1ee8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexperiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-55-ed3206edb05a>\u001b[0m in \u001b[0;36mexperiment\u001b[1;34m(partition, scaler, args)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-3cb8c74d94f0>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, partition, optimizer, loss_fn, args)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# en-decoder outputs tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# compute the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-5f158c014a1c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[1;31m# predict recursively\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m                     \u001b[0mdecoder_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                     \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-5f158c014a1c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x_input, encoder_hidden_states)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m# print(f'decoder input size {x_input.shape} to {x_input.reshape(1, -1, self.output_dim).shape}')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mrnn_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             self.weight, self.bias, bn_training, exponential_average_factor, self.eps)\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2149\u001b[0m     return torch.batch_norm(\n\u001b[1;32m-> 2150\u001b[1;33m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2151\u001b[0m     )\n\u001b[0;32m   2152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment(partition, scaler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'''\n",
    "    {df.high.max()}\n",
    "    {df.low.max()}\n",
    "    {df.open.max()}\n",
    "    {df.close.max()}\n",
    "    \n",
    "    \n",
    "    {df.high.min()}\n",
    "    {df.low.min()}\n",
    "    {df.open.min()}\n",
    "    {df.close.min()}\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ''' high - low = 변동폭 \\n'''\n",
    "    ''' 음봉양봉 구분 추가 가능'''\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
