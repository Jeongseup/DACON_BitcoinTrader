{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### library setttings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0+cpu\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# others\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "from copy import deepcopy # Add Deepcopy for args\n",
    "import pickle \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "\n",
    "# read file\n",
    "file_path = './data/merged_data.h5'\n",
    "data = pd.read_hdf(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'open': 0, 'high': 1, 'low': 2, 'close': 3, 'volume': 4, 'quote_av': 5, 'trades': 6, 'tb_base_av': 7, 'tb_quote_av': 8}\n"
     ]
    }
   ],
   "source": [
    "df = data.drop(columns = ['sample_id', 'time', 'coin_index'])\n",
    "column_indices = {name : i for i, name in enumerate(df.columns)}\n",
    "print(column_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw array shape is (1208, 1500, 10)\n"
     ]
    }
   ],
   "source": [
    "def df2d_to_array3d(df_2d):\n",
    "    \n",
    "    # 입력 받은 2차원 데이터 프레임을 3차원 numpy array로 변경하는 함수\n",
    "    feature_size = df_2d.iloc[:,2:].shape[1]\n",
    "    time_size = len(df_2d.time.value_counts())\n",
    "    sample_size = len(df_2d.sample_id.value_counts())\n",
    "    array_3d = df_2d.iloc[:,2:].values.reshape([sample_size, time_size, feature_size])\n",
    "    \n",
    "    return array_3d\n",
    "\n",
    "raw_array = df2d_to_array3d(data)\n",
    "print(f'raw array shape is {raw_array.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split array shape is (1208, 250, 10)\n"
     ]
    }
   ],
   "source": [
    "def time_split(input_array, split_size = 6):\n",
    "\n",
    "    # origin size define\n",
    "    index_size = input_array.shape[0]\n",
    "    origin_time_size = input_array.shape[1]\n",
    "    variable_size = input_array.shape[2]\n",
    "\n",
    "    # new array size define\n",
    "    new_time_size = int(origin_time_size/split_size) # 1380 / 6\n",
    "    new_array = np.zeros((index_size, new_time_size, variable_size))\n",
    "\n",
    "    for idx in range(index_size):\n",
    "        for time_idx in range(new_time_size):\n",
    "            \n",
    "\n",
    "            first_time_idx = time_idx * split_size\n",
    "            last_time_idx = ((time_idx+1) * split_size) -1\n",
    "\n",
    "            new_array[idx, time_idx, 0] = input_array[idx, first_time_idx, 0] #coin_num\n",
    "            new_array[idx, time_idx, 1] = input_array[idx, first_time_idx, 1] #open\n",
    "            \n",
    "            new_array[idx, time_idx, 2] = np.max(input_array[idx, first_time_idx:last_time_idx, 2]) #high\n",
    "            new_array[idx, time_idx, 3] = np.min(input_array[idx, first_time_idx:last_time_idx, 3]) #low\n",
    "\n",
    "            new_array[idx, time_idx, 4] = input_array[idx, last_time_idx, 4] #close\n",
    "\n",
    "            new_array[idx, time_idx, 5] = np.sum(input_array[idx, first_time_idx:last_time_idx, 5]) #etc\n",
    "            new_array[idx, time_idx, 6] = np.sum(input_array[idx, first_time_idx:last_time_idx, 6]) #etc\n",
    "            new_array[idx, time_idx, 7] = np.sum(input_array[idx, first_time_idx:last_time_idx, 7]) #etc\n",
    "            new_array[idx, time_idx, 8] = np.sum(input_array[idx, first_time_idx:last_time_idx, 8]) #etc\n",
    "            new_array[idx, time_idx, 9] = np.sum(input_array[idx, first_time_idx:last_time_idx, 9]) #etc\n",
    "\n",
    "    return new_array\n",
    "\n",
    "split_array = time_split(raw_array, split_size = 6)\n",
    "print(f'split array shape is {split_array.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ======================================================\n",
      "    Origin length is 1208, then total split length is 1208\n",
      "    ======================================================\n",
      "    train length is (845, 1500, 9),\n",
      "    val length is (242, 1500, 9),\n",
      "    test length is (121, 1500, 9),\n",
      "    num_features is (9)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def train_val_test_spliter(arr):\n",
    "    \n",
    "    \n",
    "    n = len(arr)\n",
    "    num_features = arr.shape[2] - 1\n",
    "    \n",
    "    train_arr = arr[0:int(n*0.7), :, 1:]\n",
    "    val_arr = arr[int(n*0.7):int(n*0.9), :, 1:]\n",
    "    test_arr = arr[int(n*0.9):, : , 1:]\n",
    "\n",
    "    \n",
    "    n2 = len(train_arr) + len(val_arr) + len(test_arr)\n",
    "    \n",
    "    print(\n",
    "    f'''\n",
    "    ======================================================\n",
    "    Origin length is {n}, then total split length is {n2}\n",
    "    ======================================================\n",
    "    train length is {train_arr.shape},\n",
    "    val length is {val_arr.shape},\n",
    "    test length is {test_arr.shape},\n",
    "    num_features is ({num_features})\n",
    "    '''\n",
    "    )\n",
    "    \n",
    "    return train_arr, val_arr, test_arr, num_features\n",
    "\n",
    "train_arr, val_arr, test_arr, num_features = train_val_test_spliter(raw_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = train_arr.mean(axis=(0, 1))\n",
    "train_std = train_arr.std(axis=(0, 1))\n",
    "\n",
    "ntrain_arr = (train_arr - train_mean) / train_std\n",
    "nval_arr = (val_arr - train_mean) / train_std\n",
    "ntest_arr = (test_arr - train_mean) / train_std\n",
    "\n",
    "# dataset partitioning\n",
    "\n",
    "partition = {'train': ntrain_arr, 'val':nval_arr, 'test':ntest_arr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1380, 64, 9])\n",
      "torch.Size([64, 120])\n"
     ]
    }
   ],
   "source": [
    "trainloader = DataLoader(partition['train'], batch_size = args.batch_size, shuffle = True, drop_last = True)\n",
    "\n",
    "for i, X in enumerate(trainloader):\n",
    "    print(X[:, :1380, :].transpose(0,1).shape)\n",
    "    print(X[:, 1380:, 0].shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n"
     ]
    }
   ],
   "source": [
    "# ====== initialization\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device is\",args.device)\n",
    "\n",
    "seed = 777\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# ====== Model Capacity options ===== #\n",
    "args.input_dim = 9\n",
    "args.hidden_dim = 300\n",
    "args.output_dim = 1\n",
    "args.n_layers = 2\n",
    "args.batch_size = 64\n",
    "args.dropout = 0.2\n",
    "args.use_bn = True\n",
    "\n",
    "# ====== Dataset Generating options ====== #\n",
    "args.x_frames = 1380\n",
    "args.y_frames = 120\n",
    "\n",
    "# ====== Model training options ===== #\n",
    "args.num_epoch = 30\n",
    "args.learning_rate = 0.01\n",
    "args.L2_rate = 0.0001\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, batch_size, dropout, use_bn):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.use_bn = use_bn \n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.regressor = self.make_regressor()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "    \n",
    "    def make_regressor(self):\n",
    "        layers = []\n",
    "        if self.use_bn:\n",
    "            layers.append(nn.BatchNorm1d(self.hidden_dim))\n",
    "        layers.append(nn.Dropout(self.dropout))\n",
    "        \n",
    "        layers.append(nn.Linear(self.hidden_dim, self.hidden_dim // 2))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(self.hidden_dim // 2, self.output_dim))\n",
    "        regressor = nn.Sequential(*layers)\n",
    "        return regressor\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y_pred = self.regressor(lstm_out[-1].view(self.batch_size, -1))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, partition, optimizer, loss_fn, args):\n",
    "    ''' model training '''\n",
    "   \n",
    "    # data load\n",
    "    trainloader = DataLoader(partition['train'],\n",
    "                             batch_size = args.batch_size,\n",
    "                             shuffle = True, drop_last = True)\n",
    "    \n",
    "    # model's mode setting\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(trainloader):\n",
    "    \n",
    "        X = data[:, :1380, :].transpose(0, 1).float().to(args.device)\n",
    "        y_true = data[:, 1380:, 0].float().to(args.device)\n",
    "        \n",
    "        print(X.shape, y_true.shape)\n",
    "        \n",
    "        # zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "        y_pred = model(X)\n",
    "        print(y_pred.shape)\n",
    "        \n",
    "        loss = loss_fn(y_true.view(-1), y_pred.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # get the batch loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    \n",
    "    return model, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, partition, loss_fn, args):\n",
    "    ''' model validate '''\n",
    "    \n",
    "    # data load\n",
    "    valloader = DataLoader(partition['val'], \n",
    "                           batch_size = args.batch_size, \n",
    "                           shuffle = False, drop_last = True)\n",
    "    \n",
    "    # model's mode setting\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    # evaluate\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valloader):\n",
    "            \n",
    "            X = data[:, :1380, :].transpose(0, 1).float().to(args.device)\n",
    "            y_true = data[:, 1380:, 0].float().to(args.device)\n",
    "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "            \n",
    "            # en-decoder outputs tensor \n",
    "            y_pred = model(X)\n",
    "            # compute the loss \n",
    "            loss = loss_fn(y_true.view(-1), y_pred.view(-1))\n",
    "\n",
    "            # get the batch loss\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss = val_loss / len(valloader)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, partition, scaler, args):\n",
    "    ''' model test '''\n",
    "    \n",
    "    # data load\n",
    "    testloader = DataLoader(partition['test'], \n",
    "                            batch_size = args.batch_size, \n",
    "                            shuffle = False, drop_last = True)\n",
    "    \n",
    "    # model's mode setting\n",
    "    model.eval()\n",
    "    test_mae = 0.0\n",
    "    \n",
    "    # evaluate\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader):\n",
    "            \n",
    "            X = data[:, :1380, :].transpose(0, 1).float().to(args.device)\n",
    "            y_true = data[:,1380:, 0].float().to(args.device)\n",
    "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "            \n",
    "            # en-decoder outputs tensor \n",
    "            y_pred = model(X)\n",
    "            \n",
    "#             # y values to cpu\n",
    "#             y_true = y_true.cpu().detach().numpy()\n",
    "#             y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "            # get the batch loss\n",
    "            test_mae += mean_absolute_error(y_true, y_pred)\n",
    "#             score = r2_score(y_pred = y_pred.transpose(), y_true = y_true.transpose(), multioutput = 'uniform_average')\n",
    "#             score_list.append(score)\n",
    "                        \n",
    "    test_mae /= len(testloader)\n",
    "#     score /= len(testloader) \n",
    "    return test_mae, item_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(partition, args):\n",
    "\n",
    "\n",
    "    model = LSTM(args.input_dim, args.hidden_dim, args.y_frames, args.n_layers, args.batch_size, args.dropout, args.use_bn)\n",
    "    model.to(args.device)\n",
    "    \n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.L2_rate)\n",
    "    \n",
    "    # epoch-wise loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(args.num_epoch):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model, train_loss = train(model, partition, optimizer, loss_fn, args)\n",
    "        val_loss = validate(model, partition, loss_fn, args)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # add epoch loss\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print('Epoch {},Loss(train/val) {:.3f}/{:.3f}. Took {:.2f} sec'.format(epoch+1, train_loss, val_loss, end_time-start_time))\n",
    "    \n",
    "    # test part\n",
    "    # test_mae, item_loss_list = test(model, partition, scaler, args)\n",
    "    \n",
    "    # ======= Add Result to Dictionary ======= #\n",
    "    result = {}\n",
    "    \n",
    "    result['train_losses'] = train_losses #epoch 수에 의존\n",
    "    result['val_losses'] = val_losses \n",
    "    \n",
    "    #result['test_mae'] = test_mae.round(3).item()\n",
    "    \n",
    "    # result['r2'] = np.array(score_list).mean().round(3)\n",
    "    # item_loss = np.array(item_loss_list).mean(axis=0).mean(axis=0).astype(int)\n",
    "    # item_loss = list([int(x) for x in item_loss])\n",
    "    # result['item_loss'] = item_loss\n",
    "     \n",
    "    return vars(args), result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1380, 64, 9]) torch.Size([64, 120])\n",
      "torch.Size([64, 120])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-b85b4ea3bd77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msetting\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-40-052e25c34c15>\u001b[0m in \u001b[0;36mexperiment\u001b[1;34m(partition, args)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-b7f39f6c9ed6>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, partition, optimizer, loss_fn, args)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\venv\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "setting, result = experiment(partition, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
