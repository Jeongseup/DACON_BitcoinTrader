{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35fb535a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0+cpu\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy # Add Deepcopy for args\n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import os, pickle, joblib, argparse\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from statsmodels.tsa.api import SimpleExpSmoothing\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    " \n",
    " \n",
    "print(torch.__version__)\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (8, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b0cf3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read files Complete!\n"
     ]
    }
   ],
   "source": [
    "# ================================================= #\n",
    "\n",
    "# read file\n",
    "raw_x_df = pd.read_csv('./data/train_x_df.csv')\n",
    "raw_y_df = pd.read_csv('./data/train_y_df.csv')\n",
    "\n",
    "print('Read files Complete!')\n",
    "# ================================================= #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f5e7839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================= #\n",
    "def df2d_to_array3d(df_2d):\n",
    "    \n",
    "    # 입력 받은 2차원 데이터 프레임을 3차원 numpy array로 변경하는 함수\n",
    "    feature_size = df_2d.iloc[:,2:].shape[1]\n",
    "    time_size = len(df_2d.time.value_counts())\n",
    "    sample_size = len(df_2d.sample_id.value_counts())\n",
    "    array_3d = df_2d.iloc[:,2:].values.reshape([sample_size, time_size, feature_size])\n",
    "    \n",
    "    print('DataFrame to array, Complete!')\n",
    "    \n",
    "    return array_3d\n",
    "# ================================================= #\n",
    "\n",
    "def train_tset_spliter(arr):\n",
    "    n = len(arr)\n",
    "    num_features = arr.shape[2]\n",
    "    \n",
    "    train_arr = arr[0:int(n*0.8), :, :]\n",
    "    val_arr = arr[int(n*0.8):, :, :]\n",
    "    \n",
    "    train_len = len(train_arr) \n",
    "    val_len = len(val_arr)\n",
    "    \n",
    "    print(\n",
    "    f'''\n",
    "    ======================================================\n",
    "    Origin length is {n}, then total split length is {train_len, val_len}\n",
    "    ======================================================\n",
    "    train length is {train_arr.shape},\n",
    "    val length is {val_arr.shape},\n",
    "    num_features is ({num_features})\n",
    "    '''\n",
    "    )\n",
    "    return train_arr, val_arr\n",
    "# ================================================= #\n",
    "\n",
    "def kbin_discretizer(input_array):\n",
    "\n",
    "    kb = KBinsDiscretizer(n_bins=10, strategy='uniform', encode='ordinal')\n",
    "    processed_data = np.zeros((input_array.shape[0], input_array.shape[1], 1))\n",
    "    \n",
    "    for i in range(input_array.shape[0]):\n",
    "        # coin_index_export args : (input_array, coin_num)\n",
    "        globals()['processing_array{}'.format(i)] = input_array[i,:,1]\n",
    "        \n",
    "        #globals()['outliery_array{}'.format(i)] = train_y_array[outlier[i],:,1]\n",
    "        kb.fit(globals()['processing_array{}'.format(i)].reshape(input_array.shape[1],1))\n",
    "        globals()['processed_fit{}'.format(i)] = kb.transform(globals()['processing_array{}'.format(i)].reshape(input_array.shape[1],1))\n",
    "        \n",
    "        #globals()['outliery_fit{}'.format(i)] = kb.transform(globals()['outliery_array{}'.format(i)].reshape(120,1))\n",
    "        processed_data[i,:,:] = globals()['processed_fit{}'.format(i)]\n",
    "        \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c934e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame to array, Complete!\n",
      "DataFrame to array, Complete!\n"
     ]
    }
   ],
   "source": [
    "# df to array \n",
    "raw_x_arr = df2d_to_array3d(raw_x_df)\n",
    "raw_y_arr = df2d_to_array3d(raw_y_df)\n",
    "\n",
    "# dis_x_arr = kbin_discretizer(raw_x_arr)\n",
    "# dis_y_arr = kbin_discretizer(raw_y_arr)\n",
    "\n",
    "# raw_x_arr[:, :, 1] = dis_x_arr.squeeze()\n",
    "# dis_y_arr[:, :, 1] = dis_y_arr.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc55a505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ======================================================\n",
      "    Origin length is 7661, then total split length is (6128, 1533)\n",
      "    ======================================================\n",
      "    train length is (6128, 1380, 10),\n",
      "    val length is (1533, 1380, 10),\n",
      "    num_features is (10)\n",
      "    \n",
      "\n",
      "    ======================================================\n",
      "    Origin length is 7661, then total split length is (6128, 1533)\n",
      "    ======================================================\n",
      "    train length is (6128, 120, 10),\n",
      "    val length is (1533, 120, 10),\n",
      "    num_features is (10)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train test slit\n",
    "train_x_arr, test_x_arr = train_tset_spliter(raw_x_arr)\n",
    "train_y_arr, test_y_arr = train_tset_spliter(raw_y_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04be5fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================= #\n",
    "\n",
    "def coin_index_export(input_array, coin_num):\n",
    "    ''' 함수 설명 : 코인별 인덱스 뽑기 '''\n",
    "    \n",
    "    index = []\n",
    "    sample_id_len = input_array.shape[0]\n",
    "    coin_num_col = 0 \n",
    "\n",
    "    for sample_id in range(sample_id_len):\n",
    "        if input_array[sample_id, 0, coin_num_col] == coin_num:\n",
    "            # print(sample_id)\n",
    "            index.append(sample_id)\n",
    "    return index\n",
    "\n",
    "# ================================================= #\n",
    "\n",
    "def each_coin_normalization(train_x_arr):\n",
    "    ''' 함수 설명 : 코인별 데이터 정규화 '''\n",
    "    \n",
    "    # 유니크 코인 번호\n",
    "    unique_coin_index = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    \n",
    "    #create empty scaled list\n",
    "    scaled_train_x_arr = np.zeros((train_x_arr.shape[0], train_x_arr.shape[1], train_x_arr.shape[2]))\n",
    "    \n",
    "    for temp_coin_num in unique_coin_index:\n",
    "        # 유니크 코인 번호 중 한 코인 번호씩 해당 코인에 맞는 인덱스 추출\n",
    "        # ex) if temp_coin_num is 0, temp_coin_index = [3, 7, 8, 14...]\n",
    "        temp_coin_index = coin_index_export(train_x_arr, temp_coin_num)\n",
    "        \n",
    "        # temp coin num array export\n",
    "        temp_x_arr = train_x_arr[temp_coin_index]\n",
    "        \n",
    "        # initialization\n",
    "        num_sample   = temp_x_arr.shape[0] # sample dim\n",
    "        num_sequence = temp_x_arr.shape[1] # time-sequence dim\n",
    "        num_feature  = temp_x_arr.shape[2] # feature dim\n",
    "\n",
    "        # create emptpy scaler\n",
    "        temp_scaler = MinMaxScaler()\n",
    "        \n",
    "        # 시계열을 선회하면서 피팅합니다\n",
    "        print('Current normalizing coin number is {}'.format(temp_coin_num))\n",
    "        for temp_sample, temp_index in enumerate(temp_coin_index):\n",
    "            temp_scaler.partial_fit(temp_x_arr[temp_sample, :, 5:]) # open =1, high = 2, low=3, close=4, volume=5 ~...\n",
    "\n",
    "        # 스케일링(변환)합니다.\n",
    "        for temp_sample, temp_index in enumerate(temp_coin_index):\n",
    "            scaled_train_x_arr[temp_index, :, 5:] = temp_scaler.transform(temp_x_arr[temp_sample, :, 5:]).reshape(1, num_sequence, 5)\n",
    "            scaled_train_x_arr[temp_index, :, :5] = temp_x_arr[temp_sample, :, :5]\n",
    "            \n",
    "        # save scaler for test arr\n",
    "        dir_name = './scaler'\n",
    "        file_name = f'coin_{temp_coin_num}_scaler.pkl'\n",
    "        save_path = os.path.join(dir_name, file_name)\n",
    "        joblib.dump(temp_scaler, save_path)\n",
    "        \n",
    "    \n",
    "    print(\"Each coin normalization, Complete!\")\n",
    "    return scaled_train_x_arr\n",
    "\n",
    "# ================================================= #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c14764bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current normalizing coin number is 0\n",
      "Current normalizing coin number is 1\n",
      "Current normalizing coin number is 2\n",
      "Current normalizing coin number is 3\n",
      "Current normalizing coin number is 4\n",
      "Current normalizing coin number is 5\n",
      "Current normalizing coin number is 6\n",
      "Current normalizing coin number is 7\n",
      "Current normalizing coin number is 8\n",
      "Current normalizing coin number is 9\n",
      "Each coin normalization, Complete!\n"
     ]
    }
   ],
   "source": [
    "# train_x_arr, test_x_arr = train_tset_spliter(raw_x_arr)\n",
    "# train_y_arr, test_y_arr = train_tset_spliter(raw_y_arr)\n",
    "\n",
    "# 원래 하던 All coin 방식\n",
    "# scaled_train_x_arr = each_coin_normalization(train_x_arr)\n",
    "\n",
    "# 딥러닝 홀로서기의 normaliaztion 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "738673cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================= #\n",
    "\n",
    "def simple_exponetial_smoothing_fory(arr, alpha=0.3):\n",
    "    \n",
    "    y_series = list()\n",
    "\n",
    "    for temp_arr in arr:\n",
    "        target_series = temp_arr[:, 1].reshape(-1) # open col is 1 index\n",
    "\n",
    "        smoother = SimpleExpSmoothing(target_series, initialization_method=\"heuristic\").fit(smoothing_level=alpha,optimized=False)\n",
    "        smoothing_series = smoother.fittedvalues\n",
    "\n",
    "        y_series.append(smoothing_series)\n",
    "            \n",
    "    return np.array(y_series)\n",
    "\n",
    "# ================================================= #\n",
    "\n",
    "def simple_exponetial_smoothing_forX(arr, alpha=0.3):\n",
    "    \n",
    "    # initialization\n",
    "    sample_size = int(arr.shape[0])\n",
    "    time_size = int(arr.shape[1])\n",
    "    feature_size = int(arr.shape[2])\n",
    "    \n",
    "    # create empty array\n",
    "    smoothing_arr = np.zeros((sample_size, time_size, feature_size - 1))\n",
    "\n",
    "    for idx, temp_arr in enumerate(arr):\n",
    "        for col in range(1, feature_size): # open col is 1 index\n",
    "            if col < 5:\n",
    "\n",
    "                temp_series = temp_arr[:, col].reshape(-1) \n",
    "                smoother = SimpleExpSmoothing(temp_series, initialization_method=\"heuristic\").fit(smoothing_level=0.3,optimized=False)\n",
    "                temp_smoothing_series = smoother.fittedvalues\n",
    "                smoothing_arr[idx, :, col-1] = temp_smoothing_series\n",
    "\n",
    "            else:\n",
    "                \n",
    "                pass_series = temp_arr[:, col].reshape(-1)\n",
    "                smoothing_arr[idx, :, col-1] = pass_series\n",
    "\n",
    "    return smoothing_arr\n",
    "\n",
    "# ================================================= #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b064560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train set smoothing\n",
    "# train_x_arr = simple_exponetial_smoothing_forX(train_x_arr)\n",
    "# train_y_arr = simple_exponetial_smoothing_fory(train_y_arr)\n",
    "\n",
    "# # test set smoothing \n",
    "# test_x_arr = simple_exponetial_smoothing_forX(test_x_arr)\n",
    "# print('simple exponetial smoothing Complete!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c33ee50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    train x shape is (6128, 1380, 10),\n",
      "    train y shape is (6128, 120, 10),\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'''\n",
    "    train x shape is {train_x_arr.shape},\n",
    "    train y shape is {train_y_arr.shape},\n",
    "    '''\n",
    "    \n",
    "#     test x shape is {test_x_arr.shape},\n",
    "#     test y shape is {test_y_arr.shape},\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96003793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================\n",
      "Origin length is 6128, then total split length is 5515 + 613 = 6128\n",
      "======================================================\n",
      "train X length is (5515, 1380, 9), train y length is (5515, 120),\n",
      "val X length is (613, 1380, 9), val y length is (613, 120),\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train & val set\n",
    "train_X, val_X, train_y, val_y = train_test_split(train_x_arr[:, :, 1:], train_y_arr[:, :, 1], test_size=0.1, shuffle=False)\n",
    "\n",
    "# test set\n",
    "# test_X = test_x_arr[:, :, 1:] # open col\n",
    "# test_y = test_y_arr\n",
    "\n",
    "print(\n",
    "f'''\n",
    "======================================================\n",
    "Origin length is {len(train_x_arr)}, then total split length is {len(train_X)} + {len(val_X)} = {len(train_X)+len(val_X)}\n",
    "======================================================\n",
    "train X length is {train_X.shape}, train y length is {train_y.shape},\n",
    "val X length is {val_X.shape}, val y length is {val_y.shape},\n",
    "'''\n",
    "# test X length is {test_X.shape}, test y length is {test_y.shape}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bedde48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n"
     ]
    }
   ],
   "source": [
    "# ====== initialization\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device is\",args.device)\n",
    "\n",
    "seed = 777\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# ====== Model Capacity options ===== #\n",
    "args.input_dim = 9\n",
    "args.hidden_dim = 50\n",
    "args.output_dim = 1\n",
    "args.n_layers = 1\n",
    "args.batch_size = 8\n",
    "args.dropout = 0.2\n",
    "args.use_bn = True\n",
    "\n",
    "# ====== Dataset Generating options ====== #\n",
    "args.x_frames = 255\n",
    "args.y_frames = 120\n",
    "\n",
    "# ====== Model training options ===== #\n",
    "args.num_epoch = 50\n",
    "args.learning_rate = 0.0001\n",
    "args.L2_rate = 0.00001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "94f62eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    ''' Dataset Generate'''\n",
    "    def __init__(self, X_arr, y_arr, x_frames):\n",
    "    \n",
    "        self.X_arr = X_arr\n",
    "        self.y_arr = y_arr\n",
    "        self.x_frames = x_frames\n",
    "        \n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Input indices: {self.X_arr.shape}',\n",
    "            f'Label indices: {len(self.y_arr)}',\n",
    "            f'Current column name(s): {self.x_frames}'\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_arr)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        X = self.X_arr[idx, -self.x_frames:, :]\n",
    "        X = np.log(X + 1) - np.log(X[-1, :] + 1)\n",
    "        \n",
    "        y = self.y_arr[idx, :]\n",
    "        y = np.log(y + 1) - np.log(y[-1] + 1)\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7c71beb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout, use_bn):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.use_bn = use_bn \n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "\n",
    "        self.regressor = self.make_regressor()\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_dim))\n",
    "    \n",
    "    def make_regressor(self):\n",
    "        layers = []\n",
    "        if self.use_bn:\n",
    "            layers.append(nn.BatchNorm1d(self.hidden_dim))\n",
    "        layers.append(nn.Dropout(self.dropout))\n",
    "        \n",
    "        layers.append(nn.Linear(self.hidden_dim, 200))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(200, self.output_dim))\n",
    "        regressor = nn.Sequential(*layers)\n",
    "        return regressor\n",
    "    \n",
    "    def forward(self, X):\n",
    "        lstm_out, self.hidden = self.lstm(X)\n",
    "        y_pred = self.regressor(lstm_out[-1].view(X.shape[1], -1))\n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "def train(model, partition, optimizer, loss_fn, args):\n",
    "    ''' model training '''\n",
    "   \n",
    "    # data load\n",
    "    trainloader = DataLoader(partition['train'],\n",
    "                             batch_size = args.batch_size,\n",
    "                             shuffle = True, drop_last = True)\n",
    "    \n",
    "    # model's mode setting\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for i, (X, y) in enumerate(trainloader):\n",
    "    \n",
    "        X = X.transpose(0, 1).float().to(args.device)\n",
    "        y_true = y.float().to(args.device)\n",
    "        \n",
    "#         print(X.shape, y_true.shape)\n",
    "        \n",
    "        # zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model.hidden = model.init_hidden(X.shape[1])\n",
    "\n",
    "        y_pred = model(X)\n",
    "#         print(y_pred.shape)\n",
    "        \n",
    "        loss = loss_fn(y_true.view(-1), y_pred.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # get the batch loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    # train_loss = train_loss*10E5\n",
    "    return model, train_loss\n",
    "\n",
    "\n",
    "def validate(model, partition, loss_fn, args):\n",
    "    ''' model validate '''\n",
    "    \n",
    "    # data load\n",
    "    valloader = DataLoader(partition['val'], \n",
    "                           batch_size = args.batch_size, \n",
    "                           shuffle = False, drop_last = True)\n",
    "    \n",
    "    # model's mode setting\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    # evaluate\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(valloader):\n",
    "            \n",
    "            X = X.transpose(0, 1).float().to(args.device)\n",
    "            y_true = y.float().to(args.device)\n",
    "            \n",
    "            model.hidden = model.init_hidden(X.shape[1])\n",
    "            # en-decoder outputs tensor \n",
    "            y_pred = model(X)\n",
    "            # compute the loss \n",
    "            loss = loss_fn(y_true.view(-1), y_pred.view(-1))\n",
    "\n",
    "            # get the batch loss\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss = val_loss / len(valloader)\n",
    "    # val_loss = val_loss * 10E5\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "\n",
    "def experiment(partition, args):\n",
    "\n",
    "\n",
    "    model = LSTM(args.input_dim, args.hidden_dim, args.y_frames, args.n_layers, args.dropout, args.use_bn)\n",
    "    model.to(args.device)\n",
    "    \n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.L2_rate)\n",
    "    \n",
    "    # epoch-wise loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(args.num_epoch):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model, train_loss = train(model, partition, optimizer, loss_fn, args)\n",
    "        val_loss = validate(model, partition, loss_fn, args)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # add epoch loss\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print('Epoch {},Loss(train/val) {:.3f}/{:.3f}. Took {:.2f} sec'.format(epoch+1, train_loss * 10E3, val_loss * 10E3, end_time-start_time))\n",
    "    \n",
    "    \n",
    "    # ======= Add Result to Dictionary ======= #\n",
    "    result = {}\n",
    "    \n",
    "    result['train_losses'] = train_losses #epoch 수에 의존\n",
    "    result['val_losses'] = val_losses \n",
    "     \n",
    "    return vars(args), result, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c82ac6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = WindowGenerator(train_X, train_y, x_frames = args.x_frames)\n",
    "valset = WindowGenerator(val_X, val_y, x_frames = args.x_frames)\n",
    "# testset = WindowGenerator(test_X, test_y, x_frames = args.x_frames)\n",
    "\n",
    "partition = {'train': trainset, 'val':valset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7069f619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(L2_rate=1e-05, batch_size=8, device='cpu', dropout=0.2, hidden_dim=50, input_dim=9, learning_rate=0.0001, n_layers=1, num_epoch=50, output_dim=1, use_bn=True, x_frames=255, y_frames=120)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-1629c9e68dc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msetting\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-67-db9b60bd435e>\u001b[0m in \u001b[0;36mexperiment\u001b[1;34m(partition, args)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-db9b60bd435e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, partition, optimizer, loss_fn, args)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;31m#         print(y_pred.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-db9b60bd435e>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mlstm_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    575\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[1;32m--> 577\u001b[1;33m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    578\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(args)\n",
    "setting, result, model = experiment(partition, deepcopy(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffff2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
