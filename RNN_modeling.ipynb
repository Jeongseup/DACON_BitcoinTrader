{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# others\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "from copy import deepcopy # Add Deepcopy for args\n",
    "import pickle \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (15, 9)\n",
    "\n",
    "\n",
    "# read file\n",
    "df = pd.read_hdf('./data/merged_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' greedy feature handleing'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' greedy feature handleing'''\n",
    "# test_df = train_x_df[train_x_df['volume'] != 0]\n",
    "# test_df['rest_asset'] = test_df['volume'] - test_df['tb_base_av']\n",
    "# test_df['greedy'] = test_df['tb_base_av'] / test_df['volume']\n",
    "\n",
    "# test_df2 = test_df[['time', 'coin_index', 'open', 'high', 'low', 'close', 'volume', 'trades', 'tb_base_av','rest_asset', 'greedy']]\n",
    "# test_df2[['coin_index','trades', 'volume', 'tb_base_av','rest_asset', 'greedy']].head()\n",
    "# test_df2[test_df2['greedy'] == 1][['coin_index','trades', 'volume', 'tb_base_av','rest_asset', 'greedy']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = df[df.sample_id < 100]\n",
    "# val_df = df[(df.sample_id >= 1000) & (df.sample_id < 1108)]\n",
    "# test_df = df[df.sample_id >= 1108]\n",
    "\n",
    "# # 개수 체크\n",
    "# print(\n",
    "#     f'''\n",
    "#     train set is {len(train_df) / 1500}\n",
    "#     val set is {len(val_df) / 1500}\n",
    "#     test set is {len(test_df)/1500}\n",
    "#     '''\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Generating Dataset ====== #\n",
    "\n",
    "def data_gernerate(dataframe, x_frames, y_frames, print_mode = False):\n",
    "\n",
    "    ''' 설명 생략 '''\n",
    "\n",
    "    # grouping\n",
    "    grouped_df = dataframe.groupby('sample_id')\n",
    "    \n",
    "    # export unique sample ids\n",
    "    unique_sample_id_list = grouped_df.sample_id.unique()\n",
    "\n",
    "    # create new lists\n",
    "    X, y = list(), list()\n",
    "\n",
    "    ''' 샘플 하나 선택 loop '''\n",
    "    for sample_id in unique_sample_id_list:\n",
    "        \n",
    "        # get one sample_id in sample list\n",
    "        temp_sample_id = sample_id.item()\n",
    "\n",
    "        # get one group by temp_sample_id\n",
    "        temp_df = grouped_df.get_group(temp_sample_id)\n",
    "\n",
    "        # 한 샘플당 몇 개의 arrset가 나오는 지 확인\n",
    "        count = 0\n",
    "        split_length = len(temp_df) - (x_frames + y_frames) + 1\n",
    "        \n",
    "        ''' 한 샘플 내 데이터 split loop '''\n",
    "        for time_idx in range(split_length):\n",
    "            \n",
    "            # index 변경\n",
    "            time_idx += x_frames\n",
    "            \n",
    "            # temp_data select\n",
    "            temp_arr = temp_df.iloc[time_idx - x_frames : time_idx + y_frames, 3:].values\n",
    "\n",
    "            # get values\n",
    "            temp_x = temp_arr[:x_frames, :]\n",
    "            temp_y = temp_arr[x_frames:, :]\n",
    "\n",
    "#             # 2d to 3d -> (255, 12) to (1, 255, 12) / (120, 12) to (1, 120, 12)\n",
    "#             temp_3d_x = np.expand_dims(temp_2d_x, axis = 0)\n",
    "#             temp_3d_y = np.expand_dims(temp_2d_y, axis = 0)\n",
    "            \n",
    "            # appending\n",
    "            X.append(temp_x)\n",
    "            y.append(temp_y)\n",
    "            \n",
    "            # counter printing\n",
    "            count += 1\n",
    "            if (count == split_length) & (print_mode == True):\n",
    "                print(f'현재 sample id : {temp_sample_id}')\n",
    "                print(f'{temp_sample_id}번째 sample의 생성 array수 : {count}')\n",
    "            \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_arr, train_y_arr = data_gernerate(dataframe = train_df, x_frames = 255, y_frames = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    (112600, 255, 9)\n",
      "    (112600, 120)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'''\n",
    "    {train_x_arr.shape}\n",
    "    {train_y_arr[:,:,0].shape}\n",
    "    '''\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n"
     ]
    }
   ],
   "source": [
    "import encoder_decoder\n",
    "\n",
    "# ====== initialization\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device is\",args.device)\n",
    "\n",
    "# ====== Model Capacity options ===== #\n",
    "args.input_dim = 9\n",
    "args.hidden_dim = 50\n",
    "args.output_dim = 1\n",
    "args.num_layers = 1\n",
    "args.batch_size = 64\n",
    "args.dropout = 0.2\n",
    "args.target_len = 120\n",
    "args.use_bn = True\n",
    "\n",
    "# ====== Dataset Generating options ====== #\n",
    "args.x_frames = 255\n",
    "args.y_frames = 120\n",
    "\n",
    "# ====== Model training options ===== #\n",
    "args.num_epoch = 30\n",
    "args.learning_rate = 0.001\n",
    "args.L2_rate = 0.0001\n",
    "args.training_prediction = 'mixed_teacher_forcing'\n",
    "args.teacher_forcing_ratio = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "enc = encoder_decoder.Encoder(args.input_dim, args.hidden_dim, args.num_layers)\n",
    "# Decoder\n",
    "dec = encoder_decoder.Decoder(args.output_dim, args.hidden_dim, args.num_layers, args.dropout, args.use_bn)\n",
    "\n",
    "# Seq2Seq model\n",
    "model = encoder_decoder.Seq2Seq(enc, dec, args.device, args.target_len, args.training_prediction, args.teacher_forcing_ratio)\n",
    "model.to(args.device)\n",
    "\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.L2_rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112600, 255, 9)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-61ae8c85c530>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_x_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y_arr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "for x, y in train_x_arr, train_y_arr:\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Seq2Seq' object has no attribute 'init_hidden'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-66a7d373db06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    946\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 948\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Seq2Seq' object has no attribute 'init_hidden'"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(args.num_epoch):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    model.hidden = model.init_hidden()\n",
    "    \n",
    "    for x in train_x_arr:\n",
    "        x = torch.Tensor(x).float()\n",
    "        y_true = torch.Tensor(y_true).float()\n",
    "\n",
    "        y_pred, hidden = model(x)\n",
    "        model.hidden = hidden\n",
    "\n",
    "    loss = loss_fn(y_pred.view(-1), y_true.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())\n",
    "    \n",
    "    \n",
    "        for i, (X, y) in enumerate(trainloader):\n",
    "        \n",
    "        X = X.transpose(0, 1).float().to(args.device)\n",
    "        y_true = y[:,:,0].transpose(0, 1).float().to(args.device)\n",
    "\n",
    "        # zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # en-decoder outputs tensor \n",
    "        y_pred = model(X, y_true)\n",
    "\n",
    "        # compute the loss \n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "        \n",
    "        #backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # get the batch loss\n",
    "        train_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy arrary conversion\n",
    "\n",
    "def df2d_to_array3d(df_2d):\n",
    "    # 입력 받은 2차원 데이터 프레임을 3차원 numpy array로 변경하는 함수\n",
    "    feature_size = df_2d.iloc[:,2:].shape[1]\n",
    "    time_size = len(df_2d.time.value_counts())\n",
    "    sample_size = len(df_2d.sample_id.value_counts())\n",
    "\n",
    "    # 2위 팀에서 임의로 넣어둠\n",
    "    # sample_index = df_2d.sample_id.value_counts().index\n",
    "    array_3d = df_2d.iloc[:,2:].values.reshape([sample_size, time_size, feature_size])\n",
    "    return array_3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'''\n",
    "    {df.high.max()}\n",
    "    {df.low.max()}\n",
    "    {df.open.max()}\n",
    "    {df.close.max()}\n",
    "    \n",
    "    \n",
    "    {df.high.min()}\n",
    "    {df.low.min()}\n",
    "    {df.open.min()}\n",
    "    {df.close.min()}\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ''' high - low = 변동폭 \\n'''\n",
    "    ''' 음봉양봉 구분 추가 가능'''\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
